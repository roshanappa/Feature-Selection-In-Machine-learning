{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is LDA(Linear Discreminant Analysis) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA information:-https://www.evernote.com/client/web?login=true&newReg=true#?an=true&n=4aeba86f-32b4-4dcf-b591-e67f5941c40a&s=s437&\n",
    "\n",
    "\n",
    "Playlist for LDA :-https://www.youtube.com/watch?v=CI7dIwMCRlk&t=835s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea behind the LDA is very simple,MATHEMATICALLY speaking,we need to find a new featuture space to project the data in order to maximize classes seperability.\n",
    "\n",
    "If we mind here,It is classes seperability not into the feature space,This will be the key diffrence between the LDA and PCA .LDA tries  to maximize the seperabity of classes but the PCA tries to reduce the dimensions of the features.\n",
    "\n",
    "Linear discreminant analysis is the supervised machine learning algorithms as it takes a class label into consideration for final clssification .\n",
    "\n",
    "Morever we can say LDA is kind of clasification as well.It is used to reduction dimensions in general we use at the same time  preserving as much as the class distribution information we want is possible to preserve.It will reduce the total dimension while maximizing the accuracy.\n",
    "\n",
    "    The we find the question that how the classes are been clusterized ?\n",
    "    \n",
    "Because it reducing the diamesins by reducing the similar type of information together and maximizes the projection between the classes.LDA is help to find the boundary around the cluster of the classes and project your on line so that your cluster are as seperated as possible with each cluster having relative distance to the the centroid \n",
    "\n",
    "    then the question arised that how are this cluster are define and how do we get the reduce featureset in case of a LDA ?\n",
    "    \n",
    "Basically,LDA finds the \"centroid\" of each \"class datapoints\".\n",
    "\n",
    "For an example:-with the thirteen diffrent feature LDA will find the centroid of each of its class using the thrteen diffrent features dataset.\n",
    "\n",
    "Now on the basis of this ,It determines a new diamensions which is nothing but an axis which should satisfy two criteria:-\n",
    "\n",
    "1- maximize the distance between the centroid of each class.\n",
    "\n",
    "2- minimize the variations(which LDA calls scatter and its represents by s2),with each category.\n",
    "\n",
    "Moreover, We can say that after the classification of the datasets into the several calss and each classhaving there own centroid.So the Distance of the two classes should be maximum so that we will get the better seperability.\n",
    "\n",
    "About the 2nd condition variance of each class should be maximize but it should be in its limitation of its own categories.\n",
    "\n",
    "    We can creates the maximum numbers of clusters = (Total number of classes - 1)\n",
    "\n",
    "For example if our datasets having the 2 classes we can create the only one cluster.\n",
    "\n",
    "If our datasets having multiclass problem ,if suppose we having 3 or 4 classes and for 3 class output we will get maximum 2 diamensitional output.  \n",
    "\n",
    "For that similarly,it maximum diamension are the number of components will be the always less than 1 of total avalible number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Extra Information About The LDA:-\n",
    "\n",
    "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n",
    "\n",
    "LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements.However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label).Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.\n",
    "\n",
    "LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.\n",
    "\n",
    "LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is PCA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis(PCA) :- \n",
    "\n",
    "\n",
    "Earlier we have use the Filter method,Wrapper method and Embeded method those method have been reducing dataspace into the lower dimensional dataspace. \n",
    "\n",
    "PCA is priciple componenet anlysis is a linear diamensional reduction technique that can be utilized for extracting the information from high diamensional space by projecting it into the a lower diamensional sub-space.\n",
    "\n",
    "It's tries to pressureve an essential points that have more variation of the data and remove the non-essential parts with the fewer variation.\n",
    "\n",
    "By doing so we will get the more accuracy and helps to reduce the dimensions of the features.number of \n",
    "\n",
    "    *dimensions are nothing but features that represents a data.\n",
    "    *dimensions of the feature space =number of feature avalible.\n",
    "    \n",
    "For an example:- A 28X28 image has 784 features elements(pixels) that are the dimensions or features which together represent that image.\n",
    "\n",
    "It is unsupervised diamensional reduction technique becasue we don't consider the labels.It first reduce the dimensionality of feature then by using supervised learning technique then we can apply our machine learning algorithm to do final classification on reduce dimensions of the data \n",
    "\n",
    "According to the wikipedia,\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal(perpandicular) transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "What is trying to say that, if we having data with high variance at one direction of the one arthogonal component but at the 2nd orthogonal compoenet we having the less having variance that why it will become short orthogonal component.\n",
    "\n",
    "When we use the principle orthogonal component that time we will get the varince distribution same at two orthogoanal components.\n",
    "\n",
    "\n",
    "Extra Information:-\n",
    "\n",
    "\n",
    "PCA is mostly used as a tool in exploratory data analysis(EDA) and for making predictive models. It is often used to visualize genetic distance and relatedness between populations. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after a normalization step of the initial data. The normalization of each attribute consists of mean centering – subtracting each data value from its variable's measured mean so that its empirical mean (average) is zero – and, possibly, normalizing each variable's variance to make it equal to 1; see Z-scores. The results of a PCA are usually discussed in terms of component scores, sometimes called factor scores (the transformed variable values corresponding to a particular data point), and loadings (the weight by which each standardized original variable should be multiplied to get the component score).If component scores are standardized to unit variance, loadings must contain the data variance in them (and that is the magnitude of eigenvalues). If component scores are not standardized (therefore they contain the data variance) then loadings must be unit-scaled, (\"normalized\") and these weights are called eigenvectors; they are the cosines of orthogonal rotation of variables into principal components or back.\n",
    "\n",
    "PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualised as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint[citation needed]. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.\n",
    "\n",
    "PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.\n",
    "\n",
    "PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use PCA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Data Visualization.\n",
    "\n",
    "When we having the three dimensions that time can see the data on the computer.\n",
    "Let's say we having the data with the 10 dimension then how we will visulized the data by using the help of PCA we can reduce the dimensionality of the data 2 or 3 dimensions then we can visulized the data .although that can be transform the data but we can atleast how we can visulized exactly the how it is correlated with the final traget o output.\n",
    "\n",
    "2.Speeding Machine Leaning Algorithm.\n",
    "\n",
    "Speed means if we having the 300 dimensions of our data and now you reduce teh diamesions of the data into the 2 diamensions like 1st principle and 2nd principle component.That means the total dimensions can reduce from 300 into 2 dimensions that means the training and testing will always low.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to do PCA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can calculate a principle componenet analysis on the datasets using PCA() class in scikit liberary.\n",
    "\n",
    "The benifits of this approach is that once the projection is calculated,it can be applied to new data again and again quite easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating this class,the number of components can be specified as parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is first fit on the dataset by calling fit() function,and then original datasets or other data can be projected into a subspace with choosen number of diamensions by calling the transfrom() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fit,the eigen values and principle componenets can be accessed on the PCA class via the experince_variance and components_attributes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('santander-train.csv',nrows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.170000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.030000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67333.770000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64007.970000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117310.979016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0   1     2     23                 0.0                      0.0   \n",
       "1   3     2     34                 0.0                      0.0   \n",
       "2   4     2     23                 0.0                      0.0   \n",
       "3   8     2     37                 0.0                    195.0   \n",
       "4  10     2     39                 0.0                      0.0   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "3                    195.0                      0.0                      0.0   \n",
       "4                      0.0                      0.0                      0.0   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n",
       "0                        0                        0  ...   \n",
       "1                        0                        0  ...   \n",
       "2                        0                        0  ...   \n",
       "3                        0                        0  ...   \n",
       "4                        0                        0  ...   \n",
       "\n",
       "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                      0.0                      0.0                     0.0   \n",
       "3                      0.0                      0.0                     0.0   \n",
       "4                      0.0                      0.0                     0.0   \n",
       "\n",
       "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "0                     0.0                      0.0                      0.0   \n",
       "1                     0.0                      0.0                      0.0   \n",
       "2                     0.0                      0.0                      0.0   \n",
       "3                     0.0                      0.0                      0.0   \n",
       "4                     0.0                      0.0                      0.0   \n",
       "\n",
       "   saldo_medio_var44_ult1  saldo_medio_var44_ult3          var38  TARGET  \n",
       "0                     0.0                     0.0   39205.170000       0  \n",
       "1                     0.0                     0.0   49278.030000       0  \n",
       "2                     0.0                     0.0   67333.770000       0  \n",
       "3                     0.0                     0.0   64007.970000       0  \n",
       "4                     0.0                     0.0  117310.979016       0  \n",
       "\n",
       "[5 rows x 371 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 370), (2000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= data.drop(labels='TARGET',axis=1)\n",
    "y= data['TARGET']\n",
    "\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size =.8,random_state =0,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 370), (400, 370))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quisi and constant features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 223), (400, 223))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_filter = VarianceThreshold(threshold=0.01)\n",
    "constant_filter.fit(x_train)\n",
    "x_train_filter = constant_filter.transform(x_train)\n",
    "x_test_filter = constant_filter.transform(x_test)\n",
    "\n",
    "x_train_filter.shape,x_test_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_T = x_train_filter.T\n",
    "x_test_T  = x_test_filter.T\n",
    "\n",
    "x_train_T = pd.DataFrame(x_train_T)\n",
    "x_test_T = pd.DataFrame(x_test_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_features = x_train_T.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 200), (400, 200))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_them = [not index for index in duplicated_features]\n",
    "\n",
    "\n",
    "x_train_unique = x_train_T[keep_them].T\n",
    "x_test_unique  = x_test_T[keep_them].T\n",
    "\n",
    "x_train_unique.shape,x_test_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cormat = x_train_unique.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(data,threshold):\n",
    "    corr_col = set()\n",
    "    cormat = data.corr()\n",
    "    for i in range(len(cormat.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(cormat.iloc[i,j])>threshold:\n",
    "                colname = cormat.columns[i]\n",
    "                corr_col.add(colname)\n",
    "    return corr_col\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_feature = correlation(x_train_unique,.8)\n",
    "len(correlation_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 75), (400, 75))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_uncorr = x_train_unique.drop(labels=correlation_feature,axis = 1)\n",
    "x_test_uncorr  = x_test_unique.drop(labels=correlation_feature,axis =1)\n",
    "\n",
    "\n",
    "x_train_uncorr.shape,x_test_uncorr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Dimensions Reduction By LDA or Its A Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the by running y then we will get only 0 and 1 and we had discussed that we will get the number components ways less than the 1 of numbers class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\python37-32\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "x_train_lda = lda.fit_transform(x_train_uncorr,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can see that the shape of the of x_train_uncorr is with component 1 present.Dimension of the transform data is just one.Even though the we put the value of n_components is 2 we will get the Dimension of the transform data always 1 here.\n",
    "\n",
    "Beacause the LDA tries remove the seperability of the classes not on the dimensions.It has reduce teh dimensions from 75 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avaoid the overfitting we will use the transform over the x_test also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_lda=lda.transform(x_test_uncorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest(x_train,x_test,y_train,y_test):\n",
    "    clf = RandomForestClassifier(n_jobs=-1,n_estimators=1000,random_state=0)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print('Accuracy on Test Set :-')\n",
    "    print('Accuracy :',accuracy_score(y_test,y_pred))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9125\n",
      "Wall time: 9.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "randomforest(x_train_lda,x_test_lda,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Original Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.955\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "randomforest(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we can see that the feature dimensions has been reduce by from original datasets to the tested datasets hugely.\n",
    "\n",
    "Also,we had reduced the training time and trainng time is also double here.\n",
    "\n",
    "But,Accuray slightly deteriorate that means it doesn't giving the gurantee of the accuracy but training and the featuredimension space has been been reduced from 1 to the 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Reduction By PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components here we are using 2 because PCA works on the number of features(dimensionality of features) not on the number of classes like LDA .\n",
    "\n",
    "But the PCA work on the dimensions of the features.\n",
    "\n",
    "That means we can go ahead upto (diamensions of the features -1).\n",
    "\n",
    "In the uncorr feature is 79 we can select the n_components upto the 78  in PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=42,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2,random_state=42)\n",
    "pca.fit(x_test_uncorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the training and testing set after reducing the feature using a pca transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca=  pca.transform(x_train_uncorr)\n",
    "x_test_pca=   pca.transform(x_test_uncorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had reduced the dimensionality of the features space upto the 2 here that means reduce feature space by 73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 2), (400, 2))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pca.shape,x_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "Wall time: 9.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "randomforest(x_train_pca,x_test_pca,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had got the better accuracy not so much diffrence avalible in the accuracy score if we compre with other.\n",
    "\n",
    "But we can see that training time is significantly reduced by some seconds.\n",
    "\n",
    "If we can check the training time of the original datasets that time we will get to know that traing time reduce and it can gives us the lowest traing time also.\n",
    "\n",
    "If we increas the number of component then also we willl get the better accuracy.we can increase the number of components till the 78."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see representation of the accuracy of all the components at the same time.So,for that we will be use the some python techinique.It can be done by gridseach but for simplicity we will use the for loop.\n",
    "\n",
    "let's see what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Comp 1\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 2\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 3\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 4\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 5\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 6\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 7\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 8\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 9\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 10\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 11\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 12\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 13\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 14\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 15\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 16\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 17\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 18\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 19\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 20\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 21\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 22\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 23\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 24\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 25\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 26\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 27\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 28\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 29\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 30\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 31\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 32\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 33\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 34\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 35\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 36\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 37\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 38\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 39\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 40\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 41\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 42\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 43\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 44\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 45\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 46\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 47\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 48\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 49\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 50\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 51\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 52\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 53\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 54\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 55\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 56\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 57\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 58\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 59\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 60\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 61\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 62\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 63\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 64\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 65\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 66\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 67\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 68\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 69\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 70\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 71\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 72\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 73\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 74\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 75\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 76\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 77\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n",
      "Selected Comp 78\n",
      "Accuracy on Test Set :-\n",
      "Accuracy : 0.9575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for component in range(1,79):\n",
    "    pca = PCA(n_components=2,random_state=42)\n",
    "    pca.fit(x_test_uncorr)\n",
    "    x_train_pca=  pca.transform(x_train_uncorr)\n",
    "    x_test_pca=   pca.transform(x_test_uncorr)\n",
    "    print('Selected Comp',component)\n",
    "    randomforest(x_train_pca,x_test_pca,y_train,y_test)\n",
    "    print()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
