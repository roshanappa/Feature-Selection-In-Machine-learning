{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Forward, Step Backward and Exhaustive  Feature Selection (Wrapping Method.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the wrapping method we choose the subset of feature and train the model by using this.Based on the inferences that we draw from previous model we decide to add or remove feature from our subset thats means the feature subset.\n",
    "The problem is essential reduce the search problem and this method are usually coputationally very expensive.Moreever we can say the wrapper uses the combination variable to determine the predictive power of the model and it also find the best combination of variable. It is computationally expensive than the filter method although it is quite helpfull to get the better accuracy than the filter method.It is recommanded the higher number of features.It will make the huge combination of\n",
    "feature and practically little,very difficult to implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Use the combinations of variables to determine predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Find the best combination of variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Computationally expnsive than the filter method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Perform the better than the filter method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Not recommended on high number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper methods are based on greedy search algorith as the evaluate all possible combination of feature and select the combination that produce the best result for a specific machine learning algorithms.\n",
    "\n",
    "In filtering method we have not incuded the machine learning algorithm while selecting the features but in wrapper method we include the machine learning algorithm while selecting the features.The downside of this approach is that is testing the all possible cobination of the features can be computationally very expensive particularly when when feature set is vary large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It divide into the three categories :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Subset Selection (Exhaustic Feature Selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Foreward Step Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Backward Step Selection (Recursive Feature Selection )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Foreward Step Selection or Step Foreward Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:In the first step of foreward step selection process the performace of classifier is evaluated on each of this features particularly and then best performing features are selected.\n",
    "\n",
    "Step 2:First feature tried the combination of all other feature and then the combination two features that yeild the best possible performance is selected and then process continue until we reaches our desire number features are avalible and performace.\n",
    "\n",
    "Foreward step selection method is the first work the individual feature to get the best performing feature and then We move foreward to make the best performing combination of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Backward Step Selection or Step Backward Selection(Recursive Feature Selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's name is showing that it will work exact opposite as we do in step foreward selection.\n",
    "\n",
    "Step 1:One feature is removed in round rabin fashion from the featuresets and then the performace of classifier is eveluated and this process goes on and we romoving the one feature which is least important or least performing in featurespace.It keeps on untill we reaches the desired performance,accuracy,number of featureset.\n",
    "\n",
    "This is used to balance of most important features in datasets.It works like first time we containg the large features in daatsetes in combination with other features and we keep removing those who are the least important one by one in step format and check at each wheather we got the required importance or not.\n",
    "\n",
    "If we got these feature with required then we stop removing the feature and if not then we have to keep going untill and unless we will not get best performing features combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Selection ( Exhaustive Feature Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Exhaustive feature selection method (subset selection method):Performace of machine learning algorithm is evaluated against all possible combinations of the features in the features set. The feature subset that yeild the best performances selected the exhaustive search alogorithm is the most gridy algorithm all the wrapper method.Since it tries the all combinations of features and select the best one.The downside of exaustive feature selection is that it can slower infact\n",
    "It is slower comapare to step foreward and step backward method.Since it evaluate all features combinations.\n",
    "\n",
    "lets suppose that if our modle or data have the N features then it evaluate took 2*(2^(N-1)) of feature set.\n",
    "\n",
    "For example if your model have 10  feature space then it will run atleast 124 times  and model having feature 1024 feature.\n",
    "now we see that we only having the only 10 features but our model will run for alleast 1024 times.\n",
    "\n",
    "So,this is computatinaly expensive method (exahaustive feature selcetion method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points To Remember:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Fits the modle with each possible combinations of N features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Required massive computational power.\n",
    "\n",
    "(Y=BO, Y= BO+B1X1, Y= CO+C1*X2, Y=D0+D1*X1 + D2*X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Use Test error to evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use Of mixtend in wrapper method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things has been become very easy by using the mixtend in wrapper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to install the mixtend by visisting the site otherwise we get it by running the ! pip install mixtend\n",
    "#! pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More imformation avalible at http://rasbt.github.io/mlxtend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the API of the mixtend liberary in that we get the feature selection API in that we again get SubAPI that is Sequential feature selection.\n",
    "\n",
    "It used to reduce the initial diamensinaity (d) to the k diamensinality ,\n",
    "were k is less than d (d>k)\n",
    "\n",
    "The motivation behaind the feature selection algorithm is to automatically select subset of the feature that is most \n",
    "relevant to the problem and the goal of the feature selection two float:\n",
    "\n",
    "1.we want to improve the computatinal efficiency.\n",
    "\n",
    "2.Reduce the generalization error of the model by removing the irrelevant features.\n",
    "\n",
    "In nutshell,\n",
    "\n",
    "Sequential features selection remove or add one feature at time based on the classifier performace until feature subset of the desired size k is reached.\n",
    "\n",
    "There are four diffrent flavors of SFAs(sequential feature selection algorithm) via SequentialFeatureSelector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Sequential Foreward Selection(SFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Sequential Backward Selection(SBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Sequential Foreward Floating Selection(SFFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Sequential Backward Floating Selection(SBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One by One we learn feature selection by using the above API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Foreward Selection(SFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#although we are using the tree based algorithm and it doesn't depends on the normalization and standardization of the data. \n",
    "#StandarScaler - Standardize the data on to the common scale \n",
    "#Normalization- it is techinique is useto data preparation for machine learning.Its goal is to change the numeric values columns to the common scale.\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#through which we can get the all information about the data.\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#see the Description of the wine datasets \n",
    "print(data.DESCR)\n",
    "# in that we are getting the instances(rows)of the data = 178\n",
    "#columns = 13\n",
    "#features = attributes avaliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observed at the mean of the data is varing preety much that so that we required the standardization and normalization to get the feature columnss on to the common scale.It basically required when we use the regression machine learning model to predict the data accuracy.It is preprocess of the data before going to use in model.\n",
    "\n",
    "But here,If we are not going to use the regression machine learning algorithm instad of that we are going to use classification then there is no such requirement to use of standardization and normalization.\n",
    "\n",
    "Because the classifiaction is never dependd upon the Standardization and Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame(data.data)\n",
    "y = data.target\n",
    "\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns = data.feature_names\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                         0\n",
       "malic_acid                      0\n",
       "ash                             0\n",
       "alcalinity_of_ash               0\n",
       "magnesium                       0\n",
       "total_phenols                   0\n",
       "flavanoids                      0\n",
       "nonflavanoid_phenols            0\n",
       "proanthocyanins                 0\n",
       "color_intensity                 0\n",
       "hue                             0\n",
       "od280/od315_of_diluted_wines    0\n",
       "proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,train_size =0.8,random_state=0,stratify=y)\n",
    "\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now our task reduce the 13 features space and find out the best combination of these feature set to predict the output.\n",
    "#if we havinng the large datasets with the large feature space that time its been diffict to use them as well as it will consume\n",
    "#the more time to train the model.So,for that this techinique really helpfull to select the such features those who can contribute most import the \n",
    "#predict accuracy of maodel without leading to get more training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Foreward Feature Selection(SFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here,we use the SFS = sequential feature selector in that we use the RandomForestClassfier.\n",
    "\n",
    "k_features = this number of featuers we wanted (desired number features),we use here the subset of the 6 features which are maximumly performing.\n",
    "\n",
    "foreward = this means that the we are using the foreward step selection features if its true.\n",
    "\n",
    "floating =we having two which are the plan and float here we are going to use only plan that is fload = False.\n",
    "\n",
    "verbose = we want the log while traing the model we are gone set teh true.\n",
    "\n",
    "scoring = method for the scoring we are going to use here is accuracy.\n",
    "\n",
    "n_job = how many core can while selecting the features and -1 says that use all the avaliable core in this computer.\n",
    "\n",
    "cv =cross validation.\n",
    "\n",
    "then fit the model to availd the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After the run the below code we will get the train of the model in the Backend LokyBackend with the 4 concurrent workers.\n",
    "That means it is using the four core in my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = SFS(RandomForestClassifier(n_estimators=100,random_state=0,n_jobs= -1),\n",
    "          k_features=6,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=4,\n",
    "           n_jobs=-1\n",
    "         ).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, It is step foreward feature selection technique.Now we will see that it selected the 1st a single feature out of the 6 then it find out the maximumly performace in the score format .Similarly,one by one select the feature upto the 6 with  the increasing the accuracy score.like 1/6,1/2,1/3 upto the 6/6.\n",
    "\n",
    "We can do the same thing with the 13 also.\n",
    "\n",
    "k__features = 13 then,\n",
    "\n",
    "From that we can understand that ,the if we use the large subset of feature it will no give us good accuracy by watching the senorios of the 13 feature in k_features.\n",
    "\n",
    "there wil be some subset of feature that actually giving us the better accuracy if we comapare withn the others fetaure subset.\n",
    "\n",
    "By observing the accuracy score of the feature subset, we can say that instaed of adding all feature together we some technique where we can actually subset of the features or total number of features and we ca improve the accuracy and we can reduce the total training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the maximum select feature feature_names_ by using the code :-\n",
    "\n",
    "sfs.k_feature_names_\n",
    "\n",
    "At the k_faeture pair 13/13 as well as we will accuracy also at this feature subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the the feature index of selected features by using the the below code sfs.k_feature_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.k_feature_idx_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the accuracy with the those features which are selected by using the code that is sfs.k_score_\n",
    "\n",
    "It is represnting that the maximum score can be attend by using the selected features subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.k_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and see in the pandas DataFrame throgh which we will get behaviour of the faeture accuracy changing.\n",
    "\n",
    "pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "\n",
    "It will give us the dataframe of the behaviour of that features subset one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_dict(sfs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will give us the representation of the feature subset with the respective accuracy score,feature_idx,feature_names etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing so we wil get the the represenation with the accuracy score but we can which feature subset will give us the highest accuracy than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we will get the realized that the 5 features subset given the highest accuracy among the other feature subset.\n",
    "So, we need select these five features those who are giving us the highest accuracy(avg_score)than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select these features manually also,but we do it same thing by programactically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will gives the best the performing feature subset in the range of feature (1,8).\n",
    "\n",
    "After that we will get the result with the feature subset with the accuracy as previous.\n",
    "\n",
    "Now if we see that it will give us best the performing feature so, there no need to select the best performing feature manually that we had discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs = SFS(RandomForestClassifier(n_estimators=100,random_state=0,n_jobs= -1),\n",
    "          k_features=(1,8),\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=4,\n",
    "           n_jobs=-1\n",
    "         ).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in similar fashion we have to get the best k_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.k_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run the sfs.faeture_names_ it will give us the only those features who are the best performing in (1,8).\n",
    "\n",
    "LAST time we had discuss how select the best feature set with programing so in this we got the best performing featureset.\n",
    "\n",
    "We got the same featuers those who are the best faetures had selected while we choosen the k_features =6 last time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Backward Selection (SBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here if we see that the we had done the coding with the foreward = False that means it is in backward step selection.\n",
    "\n",
    "Backward step :- it will work as feature selection by backward moment.\n",
    "\n",
    "First it will start to give  the accuracy of the all faetures subset i.e.13\n",
    "Then it will give the accuracy of the feature subset of 12th then 11th,10th and so on upto the 1st that is last feature avalible in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = SFS(RandomForestClassifier(n_jobs=-1,n_estimators=100,random_state=0),\n",
    "          k_features=(1,8),\n",
    "          floating=False,\n",
    "          forward=False,\n",
    "          verbose=2,\n",
    "          n_jobs=-1,\n",
    "          scoring='accuracy',\n",
    "          cv=4),fit(x_train,y_train)\n",
    "\n",
    "#here if we see that the we had done the coding with the foreward = False that means it is in backward step selection.\n",
    "#backward step :- it will work as feature selection by backward moment.\n",
    "#first it will start to give  the accuracy of the all faetures subset i.e.13\n",
    "#then it will give the accuracy of the feature subset of 12th then 11th,10th and so on upto the 1st that is last feature avalible in the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will train the modle in the reversed manner of the foreward step selection method.\n",
    "\n",
    "That means it will give us the first accuracy based of the all feature set 13/13 then start decreasing 12/13,11/13,10/13 and so one upto the 1/13 feature subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the sbs = sfs if we sometimes forgot to write the sbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs= sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the accuracy score\n",
    "sbs.k_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While selecting the backward step selection method we will get the feature which are most imporatnt by waching there highest avg_score.\n",
    "\n",
    "for an example we got the 6 features those contributing more accuracy in features sets.\n",
    "Because it predicted the feature selction from the bottom of the feature set.i.e.start from 1/1,2/2,3/3,4/4 upto the 13/13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the feature names.\n",
    "sbs.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,we got the optimum feature that can give us the maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Feature Selection (SFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,the exahustive feature selection techinique \n",
    "\n",
    "min_features = We wanted exahustive feature so that is why we required minimum features from where we want to make the selection.here we had taken the min_featutes =4 that means will start to predict the accuracy of the features with the faeture subset of 4.\n",
    "\n",
    "max_faetures = It gone select the feature combination UPTo featureset 5.\n",
    "\n",
    "cv = None -becasuse it is exahaustive feature selection and this is going to take lot of the time.it can take the 2 to 3 hrs to check the training time.Because exahaustive feature selection is the most exapnsive feature selection technique.It takes lot of  the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efs = EFS(RandomForestClassifier(random_state=0,n_jobs=-1,n_estimators=100),\n",
    "          min_features=4,\n",
    "          max_features=5,\n",
    "          scoring='accuracy',\n",
    "          n_jobs=-1,\n",
    "          cv=None).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can gives the output like \n",
    "feature 2002/2002 = It is representing that the model is run for the 2002 times.\n",
    "\n",
    "Mean time had calcalted the combination \n",
    "\n",
    "C(13,4) = Here we can see that the feature avalible from 4 faeture subset out of 13 feature there are possible combination is 715\n",
    "\n",
    "similarly,\n",
    "\n",
    "C(13,5) = here,we can see that the feature avalible from the 5 feature subset out of 13 feature there is possible combination is 1287\n",
    "C(13,4) + C(13,5) = 715 + 1287 If we sum this we got the 2002 features.\n",
    "\n",
    "So,from that we can say that for the min features and max feature subset it has been train for the 2002 feature subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 715 + 1287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know more about the efs then try :-\n",
    "\n",
    "help(efs)\n",
    "\n",
    "Then we will get the detail documentation on the exahaustive feature selection for classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will get the best score\n",
    "efs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will give us the best subset of features that produce the maximum result.\n",
    "\n",
    "Whatever the feature we will get these feature will gives us the result in the terms of best_score_ and \n",
    "maximum result is the one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thoght which we will get the bset performing features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " efs.best_feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the index of the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efs.best_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we want to plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting,\n",
    "\n",
    "We had imported the plot_sequential_feature_selection by uisng API plotting and refered them with the the name as plot sfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ,We are going to put the data as efs in the dictionary format and use the shaaded as std = standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_sfs(efs.get_metric_dict(),kind ='std_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will get to know homw any features are go there performace at the 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
